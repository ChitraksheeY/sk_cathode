{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do's:\n",
    "- streamline the pre/post processing with pipelines\n",
    "- clean up things, e.g. throw away train signal labels earlier for more trust\n",
    "- allow wrapped models to continue training for an extra n epochs whenever fit is called\n",
    "- maybe ensembling?\n",
    "- documentation\n",
    "\n",
    "Sanity checks:\n",
    "- epoch number matching between checkpoint name and position in loss array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from conditional_normalizing_flow import ConditionalNormalizingFlow\n",
    "from neural_network_classifier import NeuralNetworkClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :sunglasses:\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "data_path = \"./input_data/\"\n",
    "outerdata_train = np.load(join(data_path, \"outerdata_train.npy\"))\n",
    "outerdata_val = np.load(join(data_path, \"outerdata_val.npy\"))\n",
    "innerdata_train = np.load(join(data_path, \"innerdata_train.npy\"))\n",
    "innerdata_val = np.load(join(data_path, \"innerdata_val.npy\"))\n",
    "innerdata_test = np.load(join(data_path, \"innerdata_test.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data preprocessing functions\n",
    "def logit_transform(x, min_vals, max_vals):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        x_norm = (x - min_vals) / (max_vals - min_vals)\n",
    "        logit = np.log(x_norm / (1 - x_norm))\n",
    "    domain_mask = ~(np.isnan(logit).any(axis=1) | np.isinf(logit).any(axis=1))\n",
    "    return logit, domain_mask\n",
    "\n",
    "def standardize(x, mean, std):\n",
    "    return (x - mean) / std\n",
    "\n",
    "def inverse_logit_transform(x, min_vals, max_vals):\n",
    "    x_norm = 1 / (1 + np.exp(-x))\n",
    "    return x_norm * (max_vals - min_vals) + min_vals\n",
    "\n",
    "def inverse_standardize(x, mean, std):\n",
    "    return x * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow data preprocessing\n",
    "\n",
    "# the logit min/max and standard scaler parameters are derived on the outer train set\n",
    "outer_x_params = {}\n",
    "outer_x_params[\"min\"] = np.min(outerdata_train[:, 1:-1], axis=0)\n",
    "outer_x_params[\"max\"] = np.max(outerdata_train[:, 1:-1], axis=0)\n",
    "\n",
    "preprocessed_outerdata_train_x, mask = logit_transform(outerdata_train[:, 1:-1],\n",
    "                                                       outer_x_params[\"min\"],\n",
    "                                                       outer_x_params[\"max\"])\n",
    "preprocessed_outerdata_train = np.hstack([outerdata_train[:, 0:1],\n",
    "                                          preprocessed_outerdata_train_x,\n",
    "                                          outerdata_train[:, -1:]])[mask]\n",
    "\n",
    "outer_x_params[\"mean\"] = np.mean(preprocessed_outerdata_train[:, 1:-1], axis=0)\n",
    "outer_x_params[\"std\"] = np.std(preprocessed_outerdata_train[:, 1:-1], axis=0)\n",
    "preprocessed_outerdata_train[:, 1:-1] = standardize(preprocessed_outerdata_train[:, 1:-1],\n",
    "                                                    outer_x_params[\"mean\"],\n",
    "                                                    outer_x_params[\"std\"])\n",
    "\n",
    "# outer validation set if one wants to train a new flow\n",
    "preprocessed_outerdata_val_x, mask = logit_transform(outerdata_val[:, 1:-1],\n",
    "                                                     outer_x_params[\"min\"],\n",
    "                                                     outer_x_params[\"max\"])\n",
    "preprocessed_outerdata_val = np.hstack([outerdata_val[:, 0:1],\n",
    "                                        preprocessed_outerdata_val_x,\n",
    "                                        outerdata_val[:, -1:]])[mask]\n",
    "preprocessed_outerdata_val[:, 1:-1] = standardize(preprocessed_outerdata_val[:, 1:-1],\n",
    "                                                  outer_x_params[\"mean\"],\n",
    "                                                  outer_x_params[\"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new flow model from scratch\n",
    "m_train = preprocessed_outerdata_train[:, 0]\n",
    "x_train = preprocessed_outerdata_train[:, 1:-1]\n",
    "m_val = preprocessed_outerdata_val[:, 0]\n",
    "x_val = preprocessed_outerdata_val[:, 1:-1]\n",
    "\n",
    "flow_savedir = \"./trained_flows_sklearn_new/\"\n",
    "flow_model = ConditionalNormalizingFlow(save_path=flow_savedir)\n",
    "flow_model.fit(m_train, x_train, m_val, x_val, epochs=50, verbose=True)\n",
    "\n",
    "# then go back to the optimal epoch checkpoint\n",
    "flow_model.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or loading existing flow model\n",
    "flow_savedir = \"./trained_flows_sklearn/\"\n",
    "flow_model = ConditionalNormalizingFlow(save_path=flow_savedir)\n",
    "flow_model.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a KDE for the mass distribution based on the inner training set\n",
    "\n",
    "# we also perform a logit first to stretch out the hard boundaries\n",
    "m_train = innerdata_train[:, 0].reshape(-1, 1)\n",
    "inner_m_params = {\"min\": np.min(m_train),\n",
    "                  \"max\": np.max(m_train)}\n",
    "m_train, mask = logit_transform(m_train,\n",
    "                                inner_m_params[\"min\"],\n",
    "                                inner_m_params[\"max\"])\n",
    "m_train = m_train[mask]\n",
    "\n",
    "kde_model = KernelDensity(bandwidth=0.01, kernel='gaussian')\n",
    "kde_model.fit(m_train)\n",
    "\n",
    "# now let's sample 4x the number of training data\n",
    "m_samples = kde_model.sample(4*len(m_train)).astype(np.float32)\n",
    "m_samples = inverse_logit_transform(m_samples,\n",
    "                                    inner_m_params[\"min\"],\n",
    "                                    inner_m_params[\"max\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drawing samples from the flow model with the KDE samples as conditional\n",
    "x_samples = flow_model.sample(m_samples)\n",
    "\n",
    "x_samples = inverse_standardize(x_samples,\n",
    "                                outer_x_params[\"mean\"],\n",
    "                                outer_x_params[\"std\"])\n",
    "x_samples = inverse_logit_transform(x_samples,\n",
    "                                    outer_x_params[\"min\"],\n",
    "                                    outer_x_params[\"max\"])\n",
    "\n",
    "samples = np.hstack([m_samples, x_samples, np.zeros((m_samples.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing samples to inner background (idealized sanity check)\n",
    "\n",
    "for i in range(5):\n",
    "    _, binning, _ = plt.hist(innerdata_test[innerdata_test[:, -1] == 0, i],\n",
    "                             bins=100, label=\"data background\",\n",
    "                             density=True, histtype=\"step\")\n",
    "    _ = plt.hist(samples[:, i],\n",
    "                 bins=binning, label=\"sampled background\",\n",
    "                 density=True, histtype=\"step\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0, plt.gca().get_ylim()[1] * 1.2)\n",
    "    plt.xlabel(\"feature {}\".format(i))\n",
    "    plt.ylabel(\"counts\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess inner data for classifier training\n",
    "\n",
    "inner_x_params = {}\n",
    "inner_x_params[\"mean\"] = np.mean(innerdata_train[:, 1:-1], axis=0)\n",
    "inner_x_params[\"std\"] = np.std(innerdata_train[:, 1:-1], axis=0)\n",
    "preprocessed_innerdata_train_x = standardize(innerdata_train[:, 1:-1],\n",
    "                                             inner_x_params[\"mean\"],\n",
    "                                             inner_x_params[\"std\"])\n",
    "preprocessed_innerdata_train = np.hstack([innerdata_train[:, 0].reshape(-1, 1),\n",
    "                                          preprocessed_innerdata_train_x,\n",
    "                                          np.ones((len(innerdata_train), 1))])\n",
    "\n",
    "# the signal label column is replaced by a 1 for \"data\"\n",
    "preprocessed_innerdata_val_x = standardize(innerdata_val[:, 1:-1],\n",
    "                                           inner_x_params[\"mean\"],\n",
    "                                           inner_x_params[\"std\"])\n",
    "preprocessed_innerdata_val = np.hstack([innerdata_val[:, 0].reshape(-1, 1),\n",
    "                                        preprocessed_innerdata_val_x,\n",
    "                                        np.ones((len(innerdata_val), 1))])\n",
    "\n",
    "preprocessed_innerdata_test_x = standardize(innerdata_test[:, 1:-1],\n",
    "                                            inner_x_params[\"mean\"],\n",
    "                                            inner_x_params[\"std\"])\n",
    "preprocessed_innerdata_test = np.hstack([innerdata_test[:, 0].reshape(-1, 1),\n",
    "                                         preprocessed_innerdata_test_x,\n",
    "                                         innerdata_test[:, -1].reshape(-1, 1)])\n",
    "\n",
    "# same preprocessing for samples, but the signal label column is set to 0 for \"samples\"\n",
    "preprocessed_samples_x = standardize(samples[:, 1:-1],\n",
    "                                     inner_x_params[\"mean\"],\n",
    "                                     inner_x_params[\"std\"])\n",
    "preprocessed_samples = np.hstack([samples[:, 0].reshape(-1, 1),\n",
    "                                  preprocessed_samples_x,\n",
    "                                  np.zeros((len(samples), 1))])\n",
    "\n",
    "# mix data and samples into train/val sets together proportionally\n",
    "n_train = len(preprocessed_innerdata_train)\n",
    "n_val = len(preprocessed_innerdata_val)\n",
    "n_samples_train = int(n_train / (n_train + n_val) * len(preprocessed_samples))\n",
    "preprocessed_samples_train = preprocessed_samples[:n_samples_train]\n",
    "preprocessed_samples_val = preprocessed_samples[n_samples_train:]\n",
    "\n",
    "preprocessed_train_set = np.vstack([preprocessed_innerdata_train,\n",
    "                                    preprocessed_samples_train])\n",
    "preprocessed_val_set = np.vstack([preprocessed_innerdata_val,\n",
    "                                  preprocessed_samples_val])\n",
    "\n",
    "np.random.shuffle(preprocessed_train_set)\n",
    "np.random.shuffle(preprocessed_val_set)\n",
    "\n",
    "# also preprocess the test set for evaluation, but leave the signal labels untouched\n",
    "preprocessed_innerdata_test_x = standardize(innerdata_test[:, 1:-1],\n",
    "                                            inner_x_params[\"mean\"],\n",
    "                                            inner_x_params[\"std\"])\n",
    "preprocessed_innerdata_test = np.hstack([innerdata_test[:, 0].reshape(-1, 1),\n",
    "                                         preprocessed_innerdata_test_x,\n",
    "                                         innerdata_test[:, -1].reshape(-1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new NN classifier to distinguish between real inner data and samples\n",
    "\n",
    "x_train = preprocessed_train_set[:, 1:-1]\n",
    "y_train = preprocessed_train_set[:, -1]\n",
    "x_val = preprocessed_val_set[:, 1:-1]\n",
    "y_val = preprocessed_val_set[:, -1]\n",
    "\n",
    "classifier_savedir = \"trained_classifiers_sklearn_new/\"\n",
    "classifier_model = NeuralNetworkClassifier(save_path=classifier_savedir,\n",
    "                                           n_inputs=x_train.shape[1])\n",
    "classifier_model.fit(x_train, y_train, x_val, y_val,\n",
    "                     epochs=100, verbose=True)\n",
    "\n",
    "# then go back to the optimal epoch checkpoint\n",
    "classifier_model.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or loading existing classifer model\n",
    "\n",
    "classifier_savedir = \"trained_classifiers_sklearn/\"\n",
    "classifier_model = NeuralNetworkClassifier(save_path=classifier_savedir,\n",
    "                                           n_inputs=x_train.shape[1])\n",
    "classifier_model.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the signal extraction performance\n",
    "\n",
    "x_test = preprocessed_innerdata_test[:, 1:-1]\n",
    "y_test = preprocessed_innerdata_test[:, -1]\n",
    "\n",
    "preds_test = classifier_model.predict(x_test)\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds_test)\n",
    "    sic = tpr / np.sqrt(fpr)\n",
    "\n",
    "    random_tpr = np.linspace(0, 1, 300)\n",
    "    random_sic = random_tpr / np.sqrt(random_tpr)\n",
    "\n",
    "plt.plot(tpr, sic, label=\"CATHODE\")\n",
    "plt.plot(random_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CATHODEenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
