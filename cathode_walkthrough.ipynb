{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do's:\n",
    "- clean up things\n",
    "- allow wrapped models to continue training for an extra n epochs whenever fit is called\n",
    "- maybe ensembling?\n",
    "- documentation\n",
    "\n",
    "Sanity checks:\n",
    "- epoch number matching between checkpoint name and position in loss array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os.path import join\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from conditional_normalizing_flow import ConditionalNormalizingFlow\n",
    "from neural_network_classifier import NeuralNetworkClassifier\n",
    "from preprocessing import LogitScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :sunglasses:\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "data_path = \"./input_data/\"\n",
    "outerdata_train = np.load(join(data_path, \"outerdata_train.npy\"))\n",
    "outerdata_val = np.load(join(data_path, \"outerdata_val.npy\"))\n",
    "innerdata_train = np.load(join(data_path, \"innerdata_train.npy\"))\n",
    "innerdata_val = np.load(join(data_path, \"innerdata_val.npy\"))\n",
    "innerdata_test = np.load(join(data_path, \"innerdata_test.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new flow model from scratch\n",
    "\n",
    "# We streamline the preprocessing with an sklearn pipeline.\n",
    "# Ideally we would wrap the whole model, including the flow. But out of the box,\n",
    "# the pipeline class does not wrap sample() and predict_log_proba() :(\n",
    "outer_scaler = make_pipeline(LogitScaler(), StandardScaler())\n",
    "\n",
    "m_train = outerdata_train[:, 0]\n",
    "x_train = outer_scaler.fit_transform(outerdata_train[:, 1:-1])\n",
    "m_val = outerdata_val[:, 0]\n",
    "x_val = outer_scaler.transform(outerdata_val[:, 1:-1])\n",
    "\n",
    "flow_savedir = \"./trained_flows_sklearn_new/\"\n",
    "flow_model = ConditionalNormalizingFlow(save_path=flow_savedir)\n",
    "flow_model.fit(m_train, x_train, m_val, x_val, epochs=50, verbose=True)\n",
    "\n",
    "# then go back to the optimal epoch checkpoint\n",
    "flow_model.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or loading existing flow model\n",
    "\n",
    "outer_scaler = make_pipeline(LogitScaler(), StandardScaler())\n",
    "outer_scaler.fit(outerdata_train[:, 1:-1])\n",
    "\n",
    "flow_savedir = \"./trained_flows_sklearn/\"\n",
    "flow_model = ConditionalNormalizingFlow(save_path=flow_savedir)\n",
    "flow_model.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting a KDE for the mass distribution based on the inner training set\n",
    "\n",
    "# we also perform a logit first to stretch out the hard boundaries\n",
    "m_scaler = LogitScaler(epsilon=1e-8)\n",
    "m_train = m_scaler.fit_transform(innerdata_train[:, 0].reshape(-1, 1))\n",
    "\n",
    "kde_model = KernelDensity(bandwidth=0.01, kernel='gaussian')\n",
    "kde_model.fit(m_train)\n",
    "\n",
    "# now let's sample 4x the number of training data\n",
    "m_samples = kde_model.sample(4*len(m_train)).astype(np.float32)\n",
    "m_samples = m_scaler.inverse_transform(m_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drawing samples from the flow model with the KDE samples as conditional\n",
    "x_samples = flow_model.sample(m_samples)\n",
    "\n",
    "x_samples = outer_scaler.inverse_transform(x_samples)\n",
    "\n",
    "# assigning \"signal\" label 0 to samples\n",
    "samples = np.hstack([m_samples, x_samples, np.zeros((m_samples.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing samples to inner background (idealized sanity check)\n",
    "\n",
    "for i in range(5):\n",
    "    _, binning, _ = plt.hist(innerdata_test[innerdata_test[:, -1] == 0, i],\n",
    "                             bins=100, label=\"data background\",\n",
    "                             density=True, histtype=\"step\")\n",
    "    _ = plt.hist(samples[:, i],\n",
    "                 bins=binning, label=\"sampled background\",\n",
    "                 density=True, histtype=\"step\")\n",
    "    plt.legend()\n",
    "    plt.ylim(0, plt.gca().get_ylim()[1] * 1.2)\n",
    "    plt.xlabel(\"feature {}\".format(i))\n",
    "    plt.ylabel(\"counts\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning \"signal\" label 1 to data\n",
    "clsf_train_data = innerdata_train.copy()\n",
    "clsf_train_data[:, -1] = np.ones_like(clsf_train_data[:, -1])\n",
    "\n",
    "clsf_val_data = innerdata_val.copy()\n",
    "clsf_val_data[:, -1] = np.ones_like(clsf_val_data[:, -1])\n",
    "\n",
    "# then mixing data and samples into train/val sets together proportionally\n",
    "n_train = len(clsf_train_data)\n",
    "n_val = len(clsf_val_data)\n",
    "n_samples_train = int(n_train / (n_train + n_val) * len(samples))\n",
    "samples_train = samples[:n_samples_train]\n",
    "samples_val = samples[n_samples_train:]\n",
    "\n",
    "clsf_train_set = np.vstack([clsf_train_data, samples_train])\n",
    "clsf_val_set = np.vstack([clsf_val_data, samples_val])\n",
    "np.random.shuffle(clsf_train_set)\n",
    "np.random.shuffle(clsf_val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either train new NN classifier to distinguish between real inner data and samples\n",
    "\n",
    "# derive scaler parameters on data only, so it stays the same even if we resample\n",
    "inner_scaler = StandardScaler()\n",
    "inner_scaler.fit(clsf_train_data[:, 1:-1])\n",
    "\n",
    "x_train = inner_scaler.transform(clsf_train_set[:, 1:-1])\n",
    "y_train = clsf_train_set[:, -1]\n",
    "x_val = inner_scaler.transform(clsf_val_set[:, 1:-1])\n",
    "y_val = clsf_val_set[:, -1]\n",
    "\n",
    "classifier_savedir = \"trained_classifiers_sklearn_new/\"\n",
    "classifier_model = NeuralNetworkClassifier(save_path=classifier_savedir,\n",
    "                                           n_inputs=x_train.shape[1])\n",
    "classifier_model.fit(x_train, y_train, x_val, y_val,\n",
    "                     epochs=100, verbose=True)\n",
    "\n",
    "# then go back to the optimal epoch checkpoint\n",
    "classifier_model.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively load existing classifer model\n",
    "\n",
    "inner_scaler = StandardScaler()\n",
    "inner_scaler.fit(clsf_train_data[:, 1:-1])\n",
    "\n",
    "classifier_savedir = \"trained_classifiers_sklearn/\"\n",
    "classifier_model = NeuralNetworkClassifier(save_path=classifier_savedir,\n",
    "                                           n_inputs=clsf_train_set[:, 1:-1].shape[1])\n",
    "classifier_model.load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's evaluate the signal extraction performance\n",
    "\n",
    "x_test = inner_scaler.transform(innerdata_test[:, 1:-1])\n",
    "y_test = innerdata_test[:, -1]\n",
    "\n",
    "preds_test = classifier_model.predict(x_test)\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds_test)\n",
    "    sic = tpr / np.sqrt(fpr)\n",
    "\n",
    "    random_tpr = np.linspace(0, 1, 300)\n",
    "    random_sic = random_tpr / np.sqrt(random_tpr)\n",
    "\n",
    "plt.plot(tpr, sic, label=\"CATHODE\")\n",
    "plt.plot(random_tpr, random_sic, \"w:\", label=\"random\")\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"Significance Improvement\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CATHODEenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
